% ====================================================
% ================= PREAMBULO ========================
% ====================================================
\documentclass{article}
    \textheight = 21cm  % largo texto impreso
    \textwidth = 17.5cm   % ancho texto impreso
    \topmargin = -2cm   % margen superior 3-2=1cm
    \oddsidemargin = -1cm   % margen izquierdo 4.5-2=2.5cm
        % Sangr√≠a=0mm
    \parindent = 0mm
        % Otros paquetes
    \usepackage{amsmath,amssymb,amsfonts,latexsym, nicefrac}
    \usepackage[T1]{fontenc}        % fuentes adecuadas para salida
    \usepackage[latin1]{inputenc}   % acentos,etc., desde el teclado
            % Paquetes para estilos
    %\usepackage{xparse}
    %\usepackage{tikz,lipsum,lmodern}
    \usepackage[most]{tcolorbox}
    \usepackage{cancel} %Permite tachar expresiones
    
\title{Mineria de Datos}
\author{Estudiante UNED }
\date{August 2019}


% -----------Fin Preambulo ------------

% ====================================================
% ================= CUERPO ===========================
% ====================================================
\begin{document}

\maketitle

% ================ Ejercicio 1.30 ================
\begin {tcolorbox}
	% Especificaciones	
	[enhanced,attach boxed title to top left={yshift=-3mm,yshifttext=-1mm}, colback=yellow!10!white,colframe=red!60!black,colbacktitle=orange!80!black, title=Ejercicio 1,fonttitle=\bfseries, boxed title style={size=small,colframe=red!50!black} ]
	% Cuerpo caja
Calcule la divergencia de Kullback-Leibler entre dos gaussianas $p\left(x\right)=\mathcal{N}\left(x|\mu,\Sigma\right)$ y $q\left(x\right)=\mathcal{N}\left(x|m,L\right)$
	
	\begin {tcolorbox}
    	% Especificaciones
    	[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm}, colback=blue!5!white,colframe=blue!75!black,colbacktitle=red!80!black, title=1.30,fonttitle=\bfseries, boxed title style={size=small,colframe=red!50!black} ]
  		% Cuerpo caja
  		Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians $p\left(x\right)=\mathcal{N}\left(x|\mu,\Sigma\right)$ and $q\left(x\right)=\mathcal{N}\left(x|m,L\right)$
	\end{tcolorbox}
\end{tcolorbox}
$\displaystyle
    KL\left(p||q\right)=-\int{p\left(x\right)\ln{\frac{q\left(x\right)}{p\left(x\right)}}}dx \\
$
$\displaystyle
    \ln{\frac{p\left(x\right)}{q\left(x\right)}}=
    \ln{\frac{\mathcal{N}\left(x|m,L\right)}{\mathcal{N}\left(x|\mu,\Sigma\right)}}=
    \ln{\left\{\frac{\frac{1}{\left(2\pi\right)^{\nicefrac{D}{2}}}\ \frac{1}{\left|L\right|^{\nicefrac{1}{2}}}\ \exp{\left\{-\frac{1}{2}\left(x-m\right)^T L^{-1}\left(x-m\right)\right\}}}{\frac{1}{\left(2\pi\right)^{\nicefrac{D}{2}}}\ \frac{1}{\left|\Sigma\right|^{\nicefrac{1}{2}}}\exp{\left\{-\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\right\}}}\right\}}=\\
    %
    \ln{\left\{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)^\frac{1}{2}\exp{\left\{\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)-\frac{1}{2}\left(x-m\right)^TL^{-1}\left(x-m\right)\right\}}\right\}}=\\
    \frac{1}{2}\ln{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)}+\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)-\frac{1}{2}\left(x-m\right)^TL^{-1}\left(x-m\right)=\\
    -\frac{1}{2}\left\{
            \ln{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)}
            +x^T\Sigma^{-1}\left(x-\mu\right)
            -\mu^T\Sigma^{-1}\left(x-\mu\right)
            -x^T L^{-1}\left(x-m\right)
            +m^T L^{-1}\left(x-m\right)\right\}=\\
    -\frac{1}{2}\left\{
            \ln{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)}
            +x^T\Sigma^{-1}x\underbrace{-x^T\Sigma^{-1}\mu-\mu^T\Sigma^{-1}x}_{x^T\Sigma^{-1}\mu\equiv\mu^T\Sigma^{-1}x}
            +\mu^T\Sigma^{-1}\mu-x^T L^{-1}x\underbrace{+x^T L^{-1}m+m^T L^{-1}x}_{x^T L^{-1}m\equiv m^T L^{-1}x}
            -m^T L^{-1}m\right\}=\\
    -\frac{1}{2}\left\{
            \ln{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)}
            +x^T\left(\Sigma^{-1}-L^{-1}\right)x
            -2\mu^T\Sigma^{-1}x
            +\mu^T\Sigma^{-1}\mu
            +2m^TL^{-1}x
            -m^TL^{-1}m\right\}=\\
    -\frac{1}{2}\left\{
            \ln{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)}
            +x^T\left(\Sigma^{-1}+L^{-1}\right)x
            -2\left(\mu^T\Sigma^{-1}
            -m^TL^{-1}\right)x
            +\mu^T\Sigma^{-1}\mu
            -m^TL^{-1}m\right\}
$\\
$\displaystyle
KL\left(p||q\right)=-\int{p\left(x\right)\ln{\frac{q\left(x\right)}{p\left(x\right)}}}dx=\\
-\int{\mathcal{N}\left(x|\mu,\Sigma\right)\ln{\frac{\mathcal{N}\left(x|m,L\right)}{\mathcal{N}\left(x|\mu,\Sigma\right)}}}dx= -\int\mathcal{N}\left(x|\mu,\Sigma\right)dx\left(-\frac{1}{2}\right)\left\{\ln{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)}+x^T\left(\Sigma^{-1}+L^{-1}\right)x-2\left(\mu^T\Sigma^{-1}-m^TL^{-1}\right)x+\mu^T\Sigma^{-1}\mu-m^TL^{-1}m\right\}=\\
%
\frac{1}{2}\left\{
        \underbrace{\int{\ln{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)}\mathcal{N}\left(x|\mu,\Sigma\right)dx}}_{\int\mathcal{N}\left(x|\mu,\Sigma\right)dx=1}
        +\int{x^T\left(\Sigma^{-1}+L^{-1}\right)x\mathcal{N}\left(x|\mu,\Sigma\right)dx}
        +\int{-2\left(\mu^T\Sigma^{-1}-m^T L^{-1}\right)x\mathcal{N}\left(x|\mu,\Sigma\right)dx}
        +\underbrace{\int\left(\mu^T\Sigma^{-1}\mu-m^TL^{-1}m\right)\mathcal{N}\left(x|\mu,\Sigma\right)dx}_{\int\mathcal{N}\left(x|\mu,\Sigma\right)dx=1}\right\}=\\
%
\frac{1}{2}\left\{
        \ln{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)}
        +\int{x^T\left(\Sigma^{-1}+L^{-1}\right)x\mathcal{N}\left(x|\mu,\Sigma\right)dx}
        -2\left(\mu^T\Sigma^{-1}-m^T L^{-1}\right)\underbrace{\int x\mathcal{N}\left(x|\mu,\Sigma\right)dx}_\mu
        +\left(\mu^T\Sigma^{-1}\mu-m^T L^{-1}m\right)\right\}=\\
%
\frac{1}{2}\left\{
        \ln{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)}
        +\int{x^T\left(\Sigma^{-1}+L^{-1}\right)x\mathcal{N}\left(x|\mu,\Sigma\right)dx}
        -2\left(\mu^T\Sigma^{-1}-m^T L^{-1}\right)\mu
        +\left(\mu^T\Sigma^{-1}\mu-m^T L^{-1}m\right)\right\}=\\
%
\frac{1}{2}\left\{\ln{\left(\frac{\left|\Sigma\right|}{\left|L\right|}\right)}+\int{x^T\left(\Sigma^{-1}+L^{-1}\right)x\mathcal{N}\left(x|\mu,\Sigma\right)dx}+\left(2m^T L^{-1}\mu-\mu^T\Sigma^{-1}\mu-m^TL^{-1}m\right)\right\}
$
\\

% ================ Ejercicio 2.8 ================
\begin {tcolorbox}
	% Especificaciones	
	[enhanced,attach boxed title to top left={yshift=-3mm,yshifttext=-1mm}, colback=yellow!10!white,colframe=red!60!black,colbacktitle=orange!80!black, title=Ejercicio 1,fonttitle=\bfseries, boxed title style={size=small,colframe=red!50!black} ]
	% Cuerpo caja
	Considere dos variables $x$ e $y$ con distribuci\'on de probabilidad conjunta $p\left(x,y\right)$. Demuestre que:
	\begin{itemize}
    	\item $E\left[x\right]=E_y\left[E_x\left[x|y\right]\right]$
    	\item $var\left[x\right]=E_y\left[ var_{x}\left[ x|y\right]\right] + var_{y}\left[E_x\left[x|y\right]\right]$
	\end{itemize}
    	donde $E_x\left[x|y\right]$ representa el valor esperado de $x$ asumiendo la distribuci\'on de probabilidad condicionada $p\left(x|y\right)$, y una notaci\'on equivalente se utiliza para la varianza condicional.
	
	\begin {tcolorbox}
    	% Especificaciones
    	[enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm}, colback=blue!5!white,colframe=blue!75!black,colbacktitle=red!80!black, title=2.8,fonttitle=\bfseries, boxed title style={size=small,colframe=red!50!black} ]
    	% Cuerpo caja
    	Consider two variables x and y with joint distribution p(x, y). Prove the following two results
    \begin{itemize}
    	\item $E\left[x\right]=E_y\left[E_x\left[x|y\right]\right]$
    	\item $var\left[x\right]=E_y\left[ var_{x}\left[ x|y\right]\right] + var_{y}\left[E_x\left[x|y\right]\right]$
	\end{itemize}
    	Here $E_x\left[x|y\right]$ denotes the expectation of $x$ under the conditional distribution $p\left(x|y\right)$, with a similar notation for the conditional variance.
	\end{tcolorbox}
\end{tcolorbox}
$ \displaystyle
    \mathbf{E_y\left[E_x\left[x|y\right]\right]}=  
    \int_{y} \left( \int_{x}x\,p\left(x|y \right)dx \right)p\left(y \right)dy= 
    \int_{y} \int_{x}x\,p\left(x|y \right)p\left(y \right)dy\,dx =
    \int_{y} \int_{x}x\,p\left(x,y \right)dy\,dx =
    \int_{x} x \left(\int_{y}\,p\left(x,y \right)dy\right)dx =
    \int_{x} p\left(x\right)dx = \mathbf{E\left[x\right]}
$ \\

$ \displaystyle
    \mathbf{E_y\left[ var_{x}\left[ x|y\right]\right] + var_{y}\left[E_x\left[x|y\right]\right]}=\\     
    \int_{y} \left( \int_{x} \left(x-E_x\left[x|y\right]\right)^{2}\,p\left(x|y \right)dx \right)p\left(y \right)dy 
    + \int_{y} \left(\underbrace{E_y\left[E_x\left[x|y\right]\right]}_{E\left[x\right]} - E_x\left[x|y\right]\right )^{2}p\left(y \right)dy = \\  
     \int_{y} \int_{x}\left(x^{2}-2xE_x\left[x|y\right] + E\left[x|y\right]^{2}\right)\,p\left(x|y \right)p\left(y \right)dy\,dx 
      +	\int_{y} \left(E\left[x\right]^{2} -2E\left[x\right]E_x\left[x|y\right] +  E_x\left[x|y\right]^{2}\right)p\left(y \right)dy = \\     
     \int_{y} \int_{x}x^{2}\,\underbrace{p\left(x|y \right)p\left(y \right)}_{p\left(x,y \right)= p\left(x|y \right)p\left(y \right)} dy\,dx 
     -2 \int_{y}\int_{x}xE_x\left[x|y\right]\,p\left(x|y\right)p\left(y \right)dy\,dx  
     + \int_{y} \int_{x}E_x\left[x|y\right]^{2}\,\underbrace{p\left(x|y \right)p\left(y \right)}_{p\left(x,y \right)= p\left(x|y \right)p\left(y \right)}dy\,dx
     + \int_{y}E\left[x\right]^{2} p\left(y \right)dy 
     -2\int_{y} E\left[x\right]E_x\left[x|y\right]p\left(y \right)dy 
     + \int_{y}  E_x\left[x|y\right]^{2}p\left(y \right)dy=  \\    
     \int_{x}x^{2}\underbrace{\left(\int_{y} \,p\left(x,y \right)dy\right)}_{p\left(x\right)}dx 
     -2 \int_{y}E_x\left[x|y\right] \underbrace{\left(\int_{x}x\,p\left(x|y\right)\,dx\right)}_{E_x\left[x|y\right]} p\left(y \right)dy  
     + \int_{y} E_x\left[x|y\right]^{2}\underbrace{\left(\int_{x}\,p\left(x,y \right)dx\right)}_{p\left(y\right)} dy 
     + E\left[x\right]^{2} \int_{y} p\left(y \right)dy 
     -2E\left[x\right] \underbrace{\int_{y} E_x\left[x|y\right]p\left(y \right)dy}_{E_y\left[E_x\left[x|y\right]\right]} 
     + \int_{y}  E_x\left[x|y\right]^{2}p\left(y \right)dy =  \\
     \int_{x}x^{2}p\left(x\right)dx 
     -2 \int_{y}E_x\left[x|y\right] E_x\left[x|y\right] p\left(y \right)dy  
     + \int_{y} E_x\left[x|y\right]^{2}p\left(y \right)dy 
     + E\left[x\right]^{2} \int_{y} p\left(y \right)dy 
     -2E\left[x\right] \underbrace{E_y\left[E_x\left[x|y\right]\right]}_{E\left[x\right]} 
     + \int_{y}  E_x\left[x|y\right]^{2}p\left(y \right)dy = \\
     E\left[x^{2}\right] 
     \cancel{-2 \int_{y}E_x\left[x|y\right]^2 p\left(y \right)dy}  
     + \cancel{\int_{y} E_x\left[x|y\right]^{2}p\left(y \right)dy} 
     + E\left[x\right]^{2}
     -2E\left[x\right] E\left[x\right]
     + \cancel{\int_{y}  E_x\left[x|y\right]^{2}p\left(y \right)dy} = \\
     E\left[x^{2}\right] - E\left[x\right]^{2}= \mathbf{var\left[x\right]}
$ \\
\\
% ================ Ejercicio 4.8 ================
\begin {tcolorbox}
	% Especificaciones	
	[enhanced,attach boxed title to top left={yshift=-3mm,yshifttext=-1mm}, colback=yellow!10!white,colframe=red!60!black,colbacktitle=orange!80!black, title=Ejercicio 1,fonttitle=\bfseries, boxed title style={size=small,colframe=red!50!black} ]
	
	% Cuerpo caja
	Sabiendo que en un problema de clasificaci\'on con dos clases 
	$$ \displaystyle
    	p\left(C_1|x\right)=\frac{1}{1+\exp{\left(-\ln\frac{p\left(x|c_1\right)p\left(c_1\right)}{p\left(x|c_2\right)p\left(c_2\right)}\right)}}=\frac{1}{1+\exp{\left(-a\right)}}=\sigma\left(a\right)
	$$
	y suponiendo un modelo generativo en el que las verosimilitudes (likelihoods) de las dos clases vienen dadas por dos gaussianas de medias ${\mu}_1$ y ${\mu}_2$ , pero la misma varianza $\Sigma$ , demuestre que
	$$\displaystyle
		p\left(c_1|x\right)=\sigma\left(w^Tx+w_0\right)
	$$
	con $ \displaystyle w=\Sigma^{-1}\left(\mu_1-\mu_2\right)$
	$$\displaystyle
		w_0=-\frac{1}{2}\mu_1^T \Sigma^{-1}\mu_1+\frac{1}{2}\mu_2^T \Sigma^{-1}\mu_2+\ln{\frac{c_1}{c_2}}
	$$
	\begin {tcolorbox}
    	% Especificaciones
    [enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm}, colback=blue!5!white,colframe=blue!75!black,colbacktitle=red!80!black, title=4.8,fonttitle=\bfseries, boxed title style={size=small,colframe=red!50!black} ]
  
  % Cuerpo caja
Using (4.57) and (4.58), derive the result (4.65) for the posterior class probability in the two-class generative model with Gaussian densities, and verify the results (4.66) and (4.67) for the parameters w and w0.
	\end{tcolorbox}
\end{tcolorbox}

$ \displaystyle
    p\left(c_1|x\right)=\frac{p\left(x|c_1\right)p\left(c_1\right)}{p\left(x|c_1\right)p\left(c_1\right)+p\left(x|c_2\right)p\left(c_2\right)}=\frac{1}{1+\exp{\left(-a\right)}}=\sigma\left(a\right)
$  \\
$ \displaystyle
    a=\ln{
        \frac{p\left(x|c_1\right)p\left(c_1\right)}{p\left(x|c_2\right)p\left(c_2\right)}
    }
$\\
$ \displaystyle
    p\left(c_1|x\right)=\sigma\left(w^Tx+w_0\right)
$\\
$ \displaystyle
    w=\Sigma^{-1}\left(\mu_1-\mu_2\right)
$\\
$\displaystyle
    w_0=-\frac{1}{2}\mu_1^T \Sigma^{-1}\mu_1+\frac{1}{2}\mu_2^T \Sigma^{-1}\mu_2+\ln{\frac{c_1}{c_2}}
$\\

Entonces \\
$ \displaystyle
        \ln{
            \frac{p\left(x|c_1\right)p\left(c_1\right)}{p\left(x|c_2\right)p\left(c_2\right)}
        }=w^Tx+w_0
$\\
Teniendo en cuenta que: \\
$ \displaystyle
    p\left(x|c_k\right)=\frac{1}{\left(2\pi\right)^{D/2}}\frac{1}{\left|\Sigma\right|^{1/2}}\exp{
     \left\{-\frac{1}{2}\left(x-\mu_k\right)^T\Sigma^{-1}\left(x-\mu_k\right)\right\}
    }
$\\
$\displaystyle
    \ln{\frac{p\left(x|c_1\right)p\left(c_1\right)}{p\left(x|c_2\right)p\left(c_2\right)}}=\\
    \ln{\left\{\frac{\frac{1}{\left(2\pi\right)^{D/2}} \frac{1}{\left|\Sigma\right|^{1/2}} \exp{\left\{-\frac{1}{2}\left(x-\mu_1\right)^T\Sigma^{-1}\left(x-\mu_1\right)\right\}}}{\frac{1}{\left(2\pi\right)^{D/2}}\ \frac{1}{\left|\Sigma\right|^{1/2}}\ \exp{\left\{-\frac{1}{2}\left(x-\mu_2\right)^T\Sigma^{-1}\left(x-\mu_2\right)\right\}}}\right\}+\ln{\frac{p\left(c_1\right)}{p\left(c_2\right)}}}=\\
    \ln{\left\{\exp{\left\{-\frac{1}{2}\left(x-\mu_1\right)^T\Sigma^{-1}\left(x-\mu_1\right)+\frac{1}{2}\left(x-\mu_2\right)^T\Sigma^{-1}\left(x-\mu_2\right)\right\}}\right\}+\ln{\frac{p\left(c_1\right)}{p\left(c_2\right)}}}=\\
    -\frac{1}{2}\left\{x^T\Sigma^{-1}\left(x-\mu_1\right)-{\mu_1}^T\Sigma^{-1}\left(x-\mu_1\right)-x^T\Sigma^{-1}\left(x-\mu_2\right)+{\mu_2}^T\Sigma^{-1}\left(x-\mu_2\right)\right\}+\ln{\frac{p\left(c_1\right)}{p\left(c_2\right)}}=\\
    -\frac{1}{2}\left\{x^T\Sigma^{-1}x-x^T\Sigma^{-1}\mu_1-{\mu_1}^T\Sigma^{-1}x+{\mu_1}^T\Sigma^{-1}\mu_1-x^T\Sigma^{-1}x+x^T\Sigma^{-1}\mu_2+{\mu_2}^T\Sigma^{-1}x-{\mu_2}^T\Sigma^{-1}\mu_2\right\}+\ln{\frac{p\left(c_1\right)}{p\left(c_2\right)}}=\\
    -\frac{1}{2}\left\{-x^T\Sigma^{-1}\left(\mu_1-\mu_2\right)-\left(\mu_1-\mu_2\right)^T\Sigma^{-1}x+{\mu_1}^T\Sigma^{-1}\mu_1-{\mu_2}^T\Sigma^{-1}\mu_2\right\}+\ln{\frac{p\left(c_1\right)}{p\left(c_2\right)}}=\\
    -\frac{1}{2}\left\{-2\left(\mu_1-\mu_2\right)^T\Sigma^{-1}x+{\mu_1}^T\Sigma^{-1}\mu_1-{\mu_2}^T\Sigma^{-1}\mu_2\right\}+\ln{\frac{p\left(c_1\right)}{p\left(c_2\right)}}=\left(\mu_1-\mu_2\right)^T\Sigma^{-1}x-\frac{1}{2}{\mu_1}^T\Sigma^{-1}\mu_1+\frac{1}{2}{\mu_2}^T\Sigma^{-1}\mu_2+\ln{\frac{p\left(c_1\right)}{p\left(c_2\right)}}=\\
        \underbrace{
    	    \left(\Sigma^{-1}\left(\mu_1-\mu_2\right)\right)^T}_{w^T}x
         \underbrace{-\frac{1}{2}{\mu_1}^T\Sigma^{-1}\mu_1+\frac{1}{2}{\mu_2}^T\Sigma^{-1}\mu_2+\ln{\frac{p\left(c_1\right)}{p\left(c_2\right)}}}_{w_0}=\\
    w^Tx+w_0
$\\

%% ================ Ejercicio 4.10 ================
\begin {tcolorbox}
	% Especificaciones	
	[enhanced,attach boxed title to top left={yshift=-3mm,yshifttext=-1mm}, colback=yellow!10!white,colframe=red!60!black,colbacktitle=orange!80!black, title=Ejercicio 1,fonttitle=\bfseries, boxed title style={size=small,colframe=red!50!black} ]
	% Cuerpo caja
	Considere un modelo generativo de clasificaci\' on de $K$ clases definido por $K$ probabilidades a priori $p\left(C_k\right)=\pi_k $ y densidades de probabilidad del vector de caracter\' isticas de entrada $\Phi$ condicionadas a la clase $p\left(\Phi|C_k\right) $ dadas por distribuciones normales multi-variantes con la misma covarianza:
	$$ p\left(\phi_n|C_k\right)=\mathcal{N}\left(\phi_n|\mu,\Sigma\right) $$
	Suponga que se nos proporciona un conjunto de entrenamiento $\Phi_m , t_n$ donde el sub\' indice $n$ tomca valores $n=1,...,N$ y $t_n$ es un vector binario de longitud $K$ que utiliza la codificaci\' on  uno-de-$K$ (es decir, que sus componentes son $t_{n,j}=I_{j,k}$ si el patr\' on $t_n$ pertenece a la clase $C_k$). Si asumimos que el conjunto de entrenamiento constituye una muestra independiente de datos de este modelo, entonces el estimador m\' aximo-veros\' imil de las probabilidades a priori viene dado por
	$$\pi_k = \frac{N_k}{N}$$
	donde $N_k$ es el n\' umero de patrones asignados ala clase $C_k$. \\ Demuestre que el estimador m\' aximo-vers\' imil de la media de la distribuci\' pn de la clase $C_k$ viene dado por
	$$ \mu_k\ =\frac{1}{N_k}\ \sum_{n=1}^{N}{\ t_{n,k}\phi_n} $$
	y de la matriz de covarianza, viene dado por
	$$ \Sigma=\sum_{k=1}^{K}{\frac{N_k}{N}\ S_k} $$
	con
	$$ S_k=\frac{1}{N_k}\sum_{n=1}^{N}{\ t_{n,k}\left(\phi_n-\mu_k\right)\left(\phi_n-\mu_k\right)^T} $$
	\begin {tcolorbox}
   		 % Especificaciones
    [enhanced,attach boxed title to top center={yshift=-3mm,yshifttext=-1mm}, colback=blue!5!white,colframe=blue!75!black,colbacktitle=red!80!black, title=4.10,fonttitle=\bfseries, boxed title style={size=small,colframe=red!50!black} ]
  
  		% Cuerpo caja
Consider the classification model of Exercise 4.9 and now suppose that the class-conditional densities are given by Gaussian distributions with a shared covariance matrix, so that 
        $$ p\left(\phi_n|C_k\right)=\mathcal{N}\left(\phi_n|\mu,\Sigma\right) $$
Show that the maximum likelihood solution for the mean of the Gaussian distribution for class Ck is given by
        $$ \mu_k\ =\frac{1}{N_k}\ \sum_{n=1}^{N}{\ t_{n,k}\phi_n} $$
which represents the mean of those feature vectors assigned to class Ck. Similarly, show that the maximum likelihood solution for the shared covariance matrix is given by
        $$ \Sigma=\sum_{k=1}^{K}{\frac{N_k}{N}\ S_k} $$
where
        $$ S_k=\frac{1}{N_k}\sum_{n=1}^{N}{\ t_{n,k}\left(\phi_n-\mu_k\right)\left(\phi_n-\mu_k\right)^T} $$
Thus $\Sigma$ is given by a weighted average of the covariances of the data associated with each class, in which the weighting coefficients are given by the prior probabilities of the classes.
	\end{tcolorbox}
\end{tcolorbox}

$ \displaystyle
   p\left(\left\{\phi_n ,t_n\right\}|\left\{\pi_k\right\}\right)=
   \prod_{n=1}^{N}{
        \prod_{k=1}^{K}{
            \left\{p\left(\phi_n|C_k\right) p\left(C_k\right)\right\}^{t_{n,k}} 
        }
    }
$
\begin{center}
    $ \displaystyle p\left(C_k\right)=\pi_k $ \\
    $ \displaystyle p\left(\phi_n|C_k\right)=\mathcal{N}\left(\phi_n|\mu,\Sigma\right) $
\end{center}

$ \displaystyle
    \ln{p\left(\left\{\phi_n,t_n\right\}|\left\{\pi_k\right\}\right)}=\ln{
        \prod_{n=1}^{N}{
            \prod_{k=1}^{K}{
                \left\{\mathcal{N}\left(\phi_n|\mu,\Sigma\right)\ \pi_k\right\}^{t_{n,k}}
                }\
            }
        }
$
\begin{center}
    $ \displaystyle \ln{\prod_{m=1}^{M}a_m}=\ln{\sum_{m=1}^{M}a_m} $ \\
    $ \displaystyle \ln{a^n}=n\ln{a} $
\end{center}

$ \displaystyle
    \ln{
        p\left(\left\{\phi_n,t_n\right\}|\left\{\pi_k\right\}\right)
    }=
    \sum_{n=1}^{N}{
        \sum_{k=1}^{K}{t_{n,k}{
            \ln{
                \left\{\mathcal{N}\left(\phi_n|\mu,\Sigma\right)\ \pi_k\right\}}
            }
        }
    }
$
\begin{center}
    $\displaystyle \ln{\mathcal{N}\left(\phi_n|\mu,\Sigma\right)}=\ln{\left(2\pi\left|\Sigma\right|\right)^{-1/2}}-\frac{1}{2}\left(\phi_n-\mu_k\right)^T\Sigma^{-1}\left(\phi_n-\mu_k\right) $
\end{center}

$\displaystyle
    \ln{
        p\left(\left\{\phi_n,t_n\right\}|\left\{\pi_k\right\}\right)
    }=\sum_{n=1}^{N}{
        \sum_{k=1}^{K}{t_{n,k}{
            \left\{-\frac{1}{2}\ln{\left(\left|\Sigma\right|\right)}-\frac{1}{2}\left(\phi_n-\mu_k\right)^T\Sigma^{-1}\left(\phi_n-\mu_k\right)-\frac{1}{2}\ln{\left(2\pi\right)}+\ln{\pi_k}\right\}}
        }
    }
$
\begin{center}
    $\displaystyle \frac{\partial}{\partial\mu_m}\left\{\sum_{m=1}^{m}{F\left(x_m\right)\ }\right\}=\frac{\partial F\left(x_m\right)}{\partial\mu_m}$
\end{center}

$ \displaystyle
    \frac{\partial}{\partial\mu_k}\left\{\ln{p\left(\left\{\phi_n,t_n\right\}|\left\{\pi_k\right\}\right)}\right\}=\frac{\partial}{\partial\mu_k}\left\{\sum_{n=1}^{N}{t_{n,k}\left\{-\frac{1}{2}\left(\phi_n-\mu_k\right)^T\Sigma^{-1}\left(\phi_n-\mu_k\right)\right\}}\right\}=0
$

\begin{center}
    $ \displaystyle \frac{\partial}{\partial s}\left(x-s\right)^TW\left(x-s\right)=2W\left(x-s\right) $
\end{center}

$ \displaystyle
    \sum_{n=1}^{N}{t_{n,k}\ \left\{-\Sigma^{-1}\left(\phi_n-\mu_k\right)\right\}}=0\ \ \ \Longrightarrow\ \ \ -\sum_{n=1}^{N}{\ t_{n,k}\Sigma^{-1}\phi_n-t_{n,k}\Sigma^{-1}\mu_k}=0
$

\begin{center}
    $\displaystyle  \sum{a-b}=\sum a-\sum b $
\end{center}

$ \displaystyle
    \sum_{n=1}^{N}{\ t_{n,k}\Sigma^{-1}\mu_k}-\sum_{n=1}^{N}{\ t_{n,k}\Sigma^{-1}\phi_n}=0\ \ \ \Longrightarrow\ \ \ \sum_{n=1}^{N}{\ t_{n,k}\Sigma^{-1}\mu_k}=\sum_{n=1}^{N}{\ t_{n,k}\Sigma^{-1}\phi_n}
$

\begin{center}
    $ \displaystyle \Sigma \cdot \Sigma^{-1}=I $
\end{center}

$ \displaystyle
    \Sigma^{-1}\mu_k\sum_{n=1}^{N}{\ t_{n,k}}=\Sigma^{-1}\sum_{n=1}^{N}{\ t_{n,k}\phi_n}\ \Longrightarrow\ \Sigma\left\{\Sigma^{-1}\mu_k\sum_{n=1}^{N}{\ t_{n,k}}\right\}=\Sigma\left\{\Sigma^{-1}\sum_{n=1}^{N}{\ t_{n,k}\phi_n}\right\}\ \ 
$

\begin{center}
    $ \displaystyle N_k=\sum_{n=1}^{N}{\ t_{n,k}} $
\end{center}

$ \displaystyle
    \mu_k\ N_k=\sum_{n=1}^{N}{\ t_{n,k}\phi_n}\ \ \ \Longrightarrow\ \ \ \mu_k\ =\frac{1}{N_k}\ \sum_{n=1}^{N}{\ t_{n,k}\phi_n}\ 
$

\begin{center}
    $  $
\end{center}

$ \displaystyle
    \frac{\partial}{\partial\Sigma}\left\{\ln{p\left(\left\{\phi_n,t_n\right\}|\left\{\pi_k\right\}\right)}\right\}=\frac{\partial}{\partial\Sigma}\left\{\sum_{n=1}^{N}\sum_{k=1}^{K}{t_{n,k}\ \left\{-\frac{1}{2}\ln{\left(\left|\Sigma\right|\right)}-\frac{1}{2}\left(\phi_n-\mu_k\right)^T\Sigma^{-1}\left(\phi_n-\mu_k\right)\right\}}\right\}
$

\begin{center}
    $\displaystyle \sum{a+b}=\sum a+\sum b $ \\
    $\displaystyle D\left(a+b\right)=Da+Db $
\end{center}

$ \displaystyle
    -\frac{1}{2}\frac{\partial}{\partial\Sigma}\left\{\sum_{n=1}^{N}\sum_{k=1}^{K}{t_{n,k}\ \ln{\left(\left|\Sigma\right|\right)}}\right\}-\frac{1}{2}\frac{\partial}{\partial\Sigma}\left\{\sum_{n=1}^{N}\sum_{k=1}^{K}{t_{n,k}\ \left(\phi_n-\mu_k\right)^T\Sigma^{-1}\left(\phi_n-\mu_k\right)}\right\}=0
$

\begin{center}
    $ \displaystyle S_k=\frac{1}{N_k}\sum_{n=1}^{N}{\ t_{n,k}\left(\phi_n-\mu_k\right)\left(\phi_n-\mu_k\right)^T} $
\end{center}

$ \displaystyle
    -\frac{1}{2}\left\{\sum_{n=1}^{N}\sum_{k=1}^{K}{t_{n,k}\frac{\partial \ln {\left(\left|\mathrm{\Sigma}\right|\right)}}{\partial\Sigma}\ }\right\}=\frac{1}{2}
    \left\{
        \sum_{k=1}^{K}{
            \frac{\partial}{\partial\Sigma}{
                \underbrace{\sum_{n=1}^{N}{t_{n,k}\left(\phi_n-\mu_k\right)^T\mathrm{\Sigma}^{-1}\left(\phi_n-\mu_k\right)\ }}_{N_k\ Tr\left(\mathrm{\Sigma}^{-1}S_k\right)}}
        }
    \right\}
$

\begin{center}
    $\displaystyle \frac{\partial l\ n{\left(\left|A\right|\right)}}{\partial\Sigma}=\left(A^{-1}\right)^T $
\end{center}

$ \displaystyle
    -\sum_{n=1}^{N}{
        \sum_{k=1}^{K}{
            t_{n,k}{\underbrace{\left(\mathrm{\Sigma}^{-1}\right)^T}_{\mathrm{\Sigma}^{-1}}}\ 
        }
    }=\sum_{k=1}^{K}{
        \frac{\partial\left\{N_k\ Tr\left(\mathrm{\Sigma}^{-1}S_k\right)\right\}}{\partial\Sigma}
    }
$

\begin{center}
    $\displaystyle \frac{\partial\left\{\ Tr\left(AX^{-1}B\right)\right\}}{\partial X}=\left(X^{-1}B\ A\ X^{-1}\right)^T $
\end{center}

$\displaystyle
    -\sum_{n=1}^{N}\sum_{k=1}^{K}{t_{n,k}\mathrm{\Sigma}^{-1}}=-\sum_{k=1}^{K}N_k\left(\mathrm{\Sigma}^{-1}S_k\mathrm{\Sigma}^{-1}\right)^T
$

\begin{center}
    $\displaystyle \left(A\ B\ C\right)^T=C^TB^TA^T $
    $\displaystyle N=\sum_{n=1}^{N}\sum_{k=1}^{K}{t_{n,k}\ } $
\end{center}

$\displaystyle
    \mathrm{\Sigma}^{-1}\underbrace{
    \sum_{n=1}^{N}{
        \sum_{k=1}^{K}{
            t_{n,k}\ 
        }
    }
    }_N
     =\sum_{k=1}^{K}{
        N_k\left(\mathrm{\Sigma}^{-1}S_k\mathrm{\Sigma}^{-1}\right)^T
     }
$

\begin{center}
    Multiplicando por $\Sigma$ tanto por la izquierda como por la derecha ambos miembros de la ecuaci\'on
\end{center}

$\displaystyle
    \mathrm{\Sigma}\left(\mathrm{\Sigma}^{-1}N\right)\mathrm{\Sigma}=\mathrm{\Sigma}\left(\sum_{k=1}^{K}N_k\mathrm{\Sigma}^{-1}S_k\mathrm{\Sigma}^{-1}\right)\mathrm{\Sigma}
$
$\displaystyle
    N\ \mathrm{\Sigma}=\sum_{k=1}^{K}{N_kS_k}\Longrightarrow\ \ \ \Sigma=\sum_{k=1}^{K}{\frac{N_k}{N}\ S_k}
$
\end{document}
% -----------Fin Cuerpo ------------
